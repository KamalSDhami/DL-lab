{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070280cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after 'd': 'i'\n",
      "Prediction after 'i': 'v'\n",
      "Prediction after 'v': 'y'\n",
      "Prediction after 'y': 'a'\n",
      "Prediction after 'a': 'n'\n",
      "Prediction after 'n': 's'\n",
      "Prediction after 's': 'h'\n",
      "\n",
      "Generated text starting with 'd':\n",
      "divyansh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "text = \"divyansh\" * 100  # Repeat the sequence 100 times to simulate more data\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "\n",
    "input_seq = [char_to_idx[ch] for ch in text[:-1]]\n",
    "target_seq = [char_to_idx[ch] for ch in text[1:]]\n",
    "\n",
    "seq_length = 7\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(input_seq) - seq_length):\n",
    "    X.append(input_seq[i:i+seq_length])\n",
    "    y.append(target_seq[i:i+seq_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "embedding_dim = 4\n",
    "rnn_units = 16\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length),\n",
    "    SimpleRNN(units=rnn_units, return_sequences=True),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "def predict_next_char(ch):\n",
    "    input_eval = [char_to_idx[ch]] + [0] * (seq_length - 1)\n",
    "    input_eval = np.array([input_eval])\n",
    "    pred = model.predict(input_eval, verbose=0)\n",
    "    predicted_index = np.argmax(pred[0][0])\n",
    "    return idx_to_char[predicted_index]\n",
    "\n",
    "for ch in \"divyans\":\n",
    "    print(f\"Prediction after '{ch}': '{predict_next_char(ch)}'\")\n",
    "\n",
    "def generate_text(model, start_string, num_chars=7):\n",
    "    input_eval = [char_to_idx[ch] for ch in start_string]\n",
    "    input_eval = np.array(input_eval).reshape(1, -1)\n",
    "    text_generated = []\n",
    "\n",
    "    for _ in range(num_chars):\n",
    "        predictions = model.predict(input_eval, verbose=0)\n",
    "        predicted_id = np.argmax(predictions[0][-1])\n",
    "        text_generated.append(idx_to_char[predicted_id])\n",
    "        input_eval = np.append(input_eval[0], predicted_id)[-seq_length:].reshape(1, -1)\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nGenerated text starting with 'd':\")\n",
    "print(generate_text(model, start_string='d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec6613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after 'd': 'i'\n",
      "Prediction after 'i': 'v'\n",
      "Prediction after 'v': 'y'\n",
      "Prediction after 'y': 'a'\n",
      "Prediction after 'a': 'n'\n",
      "Prediction after 'n': 's'\n",
      "Prediction after 's': 'h'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">336</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │           \u001b[38;5;34m336\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m8\u001b[0m)           │           \u001b[38;5;34m136\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,514</span> (5.92 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,514\u001b[0m (5.92 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">504</span> (1.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m504\u001b[0m (1.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> (3.95 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,010\u001b[0m (3.95 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text starting with 'd':\n",
      "divyansh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "text = \"divyansh\" * 100  # Repeat the sequence 100 times to simulate more data\n",
    "chars = sorted(set(text))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "\n",
    "input_seq = [char_to_idx[ch] for ch in text[:-1]]\n",
    "target_seq = [char_to_idx[ch] for ch in text[1:]]\n",
    "\n",
    "seq_length = 7\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(input_seq) - seq_length):\n",
    "    X.append(input_seq[i:i+seq_length])\n",
    "    y.append(target_seq[i:i+seq_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "embedding_dim = 4\n",
    "rnn_units = 16\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length),\n",
    "    SimpleRNN(units=rnn_units, return_sequences=True),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "def predict_next_char(ch):\n",
    "    input_eval = [char_to_idx[ch]] + [0] * (seq_length - 1)\n",
    "    input_eval = np.array([input_eval])\n",
    "    pred = model.predict(input_eval, verbose=0)\n",
    "    predicted_index = np.argmax(pred[0][0])\n",
    "    return idx_to_char[predicted_index]\n",
    "\n",
    "for ch in \"divyans\":\n",
    "    print(f\"Prediction after '{ch}': '{predict_next_char(ch)}'\")\n",
    "\n",
    "def generate_text(model, start_string, num_chars=7):\n",
    "    input_eval = [char_to_idx[ch] for ch in start_string]\n",
    "    input_eval = np.array(input_eval).reshape(1, -1)\n",
    "    text_generated = []\n",
    "\n",
    "    for _ in range(num_chars):\n",
    "        predictions = model.predict(input_eval, verbose=0)\n",
    "        predicted_id = np.argmax(predictions[0][-1])\n",
    "        text_generated.append(idx_to_char[predicted_id])\n",
    "        input_eval = np.append(input_eval[0], predicted_id)[-seq_length:].reshape(1, -1)\n",
    "\n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nGenerated text starting with 'd':\")\n",
    "print(generate_text(model, start_string='d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84b9f52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract hidden states from the RNN output (before Dense layer)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m extract_hidden_model \u001b[38;5;241m=\u001b[39m model(inputs\u001b[38;5;241m=\u001b[39m\u001b[43minput_layer\u001b[49m, outputs\u001b[38;5;241m=\u001b[39mrnn_output)\n\u001b[0;32m      3\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m extract_hidden_model\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Print intermediate hidden states\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_layer' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract hidden states from the RNN output (before Dense layer)\n",
    "extract_hidden_model = model(inputs=input_layer, outputs=rnn_output)\n",
    "hidden_states = extract_hidden_model.predict(X)\n",
    "\n",
    "# Print intermediate hidden states\n",
    "print(\"\\nIntermediate Hidden States:\")\n",
    "for t in range(len(input_chars)):\n",
    "    print(f\"{input_chars[t]} → {np.round(hidden_states[t][0], 3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4555a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden layer\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Dense \n",
    "from tensorflow.keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1140e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001A9A2106340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\n",
      "Intermediate Hidden States:\n",
      "H → [ 0.617 -0.467 -0.661 -0.053  0.678  0.475  0.607 -0.713]\n",
      "E → [ 0.197  0.67   0.445  0.304  0.493 -0.195  0.261 -0.663]\n",
      "L → [-0.466 -0.253  0.18  -0.291  0.833 -0.595  0.511 -0.137]\n",
      "L → [-0.466 -0.253  0.18  -0.291  0.833 -0.595  0.511 -0.137]\n",
      "WARNING:tensorflow:6 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001A9A21074C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\n",
      "Predicted Next Characters:\n",
      "H → E\n",
      "E → L\n",
      "L → L\n",
      "L → L\n"
     ]
    }
   ],
   "source": [
    "# 1. Text and character mapping\n",
    "text = \"HELLO\"\n",
    "chars = sorted(set(text))  # Unique characters: ['E', 'H', 'L', 'O']\n",
    "char_to_index = {c: i for i, c in enumerate(chars)}\n",
    "index_to_char = {i: c for c, i in char_to_index.items()}\n",
    "\n",
    "# 2. Prepare input and target sequences\n",
    "input_chars = text[:-1]   # 'HELL'\n",
    "target_chars = text[1:]   # 'ELLO'\n",
    "\n",
    "input_indices = [char_to_index[c] for c in input_chars]\n",
    "target_indices = [char_to_index[c] for c in target_chars]\n",
    "\n",
    "# 3. One-hot encode and reshape\n",
    "X = to_categorical(input_indices, num_classes=len(chars))\n",
    "X = np.reshape(X, (len(X), 1, len(chars)))  # Shape: (4, 1, 4)\n",
    "\n",
    "y = to_categorical(target_indices, num_classes=len(chars))\n",
    "y = np.reshape(y, (len(y), 1, len(chars)))  # Shape: (4, 1, 4)\n",
    "\n",
    "# 4. Define the model\n",
    "input_layer = Input(shape=(1, len(chars)))\n",
    "rnn_output, final_state = SimpleRNN(8, return_sequences=True, return_state=True)(input_layer)\n",
    "output = Dense(len(chars), activation='softmax')(rnn_output)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 5. Train the model\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "# 6. Extract hidden states\n",
    "extract_hidden_model = Model(inputs=input_layer, outputs=rnn_output)\n",
    "hidden_states = extract_hidden_model.predict(X)\n",
    "\n",
    "# 7. Print intermediate hidden states\n",
    "print(\"\\nIntermediate Hidden States:\")\n",
    "for t in range(len(input_chars)):\n",
    "    print(f\"{input_chars[t]} → {np.round(hidden_states[t][0], 3)}\")\n",
    "\n",
    "# 8. Predict next characters\n",
    "predictions = model.predict(X)\n",
    "print(\"\\nPredicted Next Characters:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    predicted_index = np.argmax(pred[0])\n",
    "    print(f\"{input_chars[i]} → {index_to_char[predicted_index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2880f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for next word from sentence\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db56495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "Artificial intelligence is → transforming\n",
      "It helps in → intelligence\n",
      "RNNs are used → intelligence\n",
      "language processing for → intelligence\n"
     ]
    }
   ],
   "source": [
    "# Text Data\n",
    "paragraph = \"\"\" \n",
    "Artificial intelligence is transforming the world. It helps in automating tasks,  \n",
    "enhancing productivity, and discovering new solutions to complex problems. \n",
    "RNNs are used in natural language processing for sequence prediction tasks. \n",
    "\"\"\"\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([paragraph])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Generate input sequences\n",
    "input_sequences = []\n",
    "token_list = tokenizer.texts_to_sequences([paragraph])[0]\n",
    "\n",
    "for i in range(1, len(token_list)):\n",
    "    n_gram_sequence = token_list[:i + 1]\n",
    "    input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')\n",
    "\n",
    "# Split into input (X) and output (y)\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_seq_len - 1))\n",
    "model.add(SimpleRNN(64))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "\n",
    "# Prediction Function (Fixed)\n",
    "def predict_next_word(model, tokenizer, text_seed, max_seq_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text_seed])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
    "    predicted_probs = model.predict(token_list, verbose=0)\n",
    "    predicted_index = np.argmax(predicted_probs)\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_index:\n",
    "            return word\n",
    "    return \"\"\n",
    "\n",
    "# Test Predictions\n",
    "test_phrases = [\n",
    "    \"Artificial intelligence is\",\n",
    "    \"It helps in\",\n",
    "    \"RNNs are used\",\n",
    "    \"language processing for\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for phrase in test_phrases:\n",
    "    next_word = predict_next_word(model, tokenizer, phrase, max_seq_len)\n",
    "    print(f\"{phrase} → {next_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04a265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
