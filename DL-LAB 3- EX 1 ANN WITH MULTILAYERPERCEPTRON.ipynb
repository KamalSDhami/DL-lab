{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c1adfd-4da3-4563-a53b-1f70ce77fe8e",
   "metadata": {},
   "source": [
    "# Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same using XOR Gate. Vary the activation functions and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbeaf05d-0549-439a-8902-2a9e0fdf7343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Predicted Output: [0.00022736]\n",
      "Input: [0 1], Predicted Output: [0.99990944]\n",
      "Input: [1 0], Predicted Output: [0.99989194]\n",
      "Input: [1 1], Predicted Output: [6.39046223e-05]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layers, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes a multi-layer perceptron.\n",
    "        :param layers: List containing the number of neurons in each layer.\n",
    "        :param learning_rate: Learning rate for weight updates.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.weights.append(np.random.randn(layers[i], layers[i + 1]) * 0.1)\n",
    "            self.biases.append(np.random.randn(layers[i + 1]) * 0.1)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        :param x: Input data.\n",
    "        :return: Outputs of each layer.\n",
    "        \"\"\"\n",
    "        activations = [x]\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            x = self.relu(np.dot(x, self.weights[i]) + self.biases[i])\n",
    "            activations.append(x)\n",
    "\n",
    "        # Output layer uses sigmoid for classification\n",
    "        x = self.sigmoid(np.dot(x, self.weights[-1]) + self.biases[-1])\n",
    "        activations.append(x)\n",
    "        return activations\n",
    "\n",
    "    def backward(self, activations, y):\n",
    "        \"\"\"\n",
    "        Backward pass using backpropagation.\n",
    "        :param activations: Outputs from forward pass.\n",
    "        :param y: True labels.\n",
    "        \"\"\"\n",
    "        deltas = [activations[-1] - y]  # Error at output layer\n",
    "\n",
    "        # Compute gradients for hidden layers\n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            deltas.append(deltas[-1] @ self.weights[i + 1].T * self.relu_derivative(activations[i + 1]))\n",
    "\n",
    "        deltas.reverse()\n",
    "\n",
    "        # Update weights and biases\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * np.outer(activations[i], deltas[i])\n",
    "            self.biases[i] -= self.learning_rate * deltas[i]\n",
    "\n",
    "    def train(self, X, y, epochs=10000):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                activations = self.forward(X[i])\n",
    "                self.backward(activations, y[i])\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)[-1]\n",
    "\n",
    "# Example usage (XOR problem)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y = np.array([[0], [1], [1], [0]])  # XOR outputs\n",
    "\n",
    "mlp = MLP(layers=[2, 4, 1], learning_rate=0.1)  # 2 input neurons, 4 hidden neurons, 1 output neuron\n",
    "mlp.train(X, y, epochs=10000)\n",
    "\n",
    "# Testing\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]}, Predicted Output: {mlp.predict(X[i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33197e-b9e5-4f62-b7c5-e3354c4400b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12b6a2-0f47-4526-8799-1b383999d5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46d60977-1fe6-4a2d-b96e-24837f052853",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1721418029.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-Layer Structure:\n",
    "\n",
    "Input Layer → Hidden Layers (ReLU activation) → Output Layer (Sigmoid for classification).\n",
    "Number of neurons per layer is defined in layers=[2, 4, 1].\n",
    "Forward Pass:\n",
    "\n",
    "Computes activations layer by layer using ReLU for hidden layers.\n",
    "Uses Sigmoid for the final output (for binary classification).\n",
    "Backward Pass (Backpropagation):\n",
    "\n",
    "Computes error at output layer.\n",
    "Propagates error backward through hidden layers.\n",
    "Updates weights and biases using gradient descent.\n",
    "Training on XOR Problem:\n",
    "\n",
    "Unlike a single-layer perceptron, this MLP can solve XOR due to the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1770cd3-fc23-4b07-a92e-27c1433c3bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
